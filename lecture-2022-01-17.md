---
instructor: Prof. Dr. Peter Thiemann
tutor: Fabian Krause
date: 2022-01-17
title: Bootstrapping
---

# assigned reading: none!

this week we're going to look at

* bootstrapping (aka writing the compiler from scratch)
* frontend stuff (lexing and parsing)

# bootstrapping

Suggested reading:

1.  Jay Earley, Howard E. Sturgis. A formalism for translator interactions.
	Commun. ACM 13(10): 607-617 (1970).
   <https://www.semanticscholar.org/paper/A-formalism-for-translator-interactions-Earley-Sturgis/f8535523d3bb7b35aaffbc06f0c5106a2332d582>
2.  Andrew W. Appel. Axiomatic Bootstrapping: A Guide for Compiler Hackers.
	ACM Trans. Program. Lang. Syst. 16(6): 1699-1718 (1994).
	<https://www.cs.princeton.edu/research/techreps/TR-451-94>

This presentation is based on (1) and the following notes refer to
section numbers in that paper.



# 2. The Notation

We write `L[p] (in)` for the result of running the `L`-program `p` on
input `in`. 



More precisely what we mean by *L2->L3 is a translator from L2 to L3
written in L1*: (This is a simple preview of material in section 5 of
the paper).

* If the input of L2->L3 is a (syntactically) correct L2-program,
    then the output is a (syntactically) correct L3-program.
* If `p2` is a L2-program, `in` is an input to `p2`, and `p3 = L1[L2->L3](p2)`, then
      `L2[p2](in)` = `L3[p3](in)`.

Similarly, let `intL2` be an interpreter for L2 written in L1:

* If `L2[p2](in) = out`, then `L1[intL2] (p2, in) = out`.


Today, the language Algol is mostly of historic interest.
A present time example would be the language C.

Similary, a modern time analogon to JOSS would be Python.










# 3. Some Examples

There are several modern time analoga to IPL.

* Java is first compiled to intermediate code for the Java Virtual
  Machine (JVM), which is stored in class files. This code is conceptually interpreted, but most  JVM
  implementations perform JIT (just in time) compilation and run the resulting
  compiled code.
* JavaScript is translated to bytecode, which is first interpreted,
  and JIT compiled if it's executed sufficiently often.
* OCaml has a bytecode compiler, the output of which is interpreted at
  run time. There is also a native compiler, which compiles directly
  to machine code (as with C above).

The reason for the intermediate step is (usually) portability: To get
an (OCaml) program running on a new machine, we only need to run the
bytecode interpreter on the new machine. Porting the interpreter is
usually easier than porting the compiler.

The next example concerns a *compiler-compiler*.

## compiler-compiler

One approach to a compiler-compiler is as follows.
As there are many common tasks in compilers, one might conceive a
*domain-specific language*, here called CWL, in which to write the compiler so that
common tasks (e.g., dataflow analyses, typing) can be expressed at
a high level.

In the concrete example, we write a specification to compile SL->ML
in the CWL language and use the CWL compiler to obtain an SL->ML
compiler in ML, which is readily executable.

The FSL example makes use of what we would call a scanner/parser
generator today: there is one phase (PL) that handles the syntax and
another (FSL) that handles code generation.

Clearly, there is no need to compile CWL, but it could be interpreted
to the same effect!

The Lisp example demonstrates how we can bootstrap a compiler! The
ingredients are
* a Lisp interpreter written somehow in ML (either directly in ML or
  in some other language, for which a compiler to ML already exists).
* a Lisp compiler Lisp->ML written in Lisp (!)

In fact, John McCarthy, the inventor of Lisp, says <http://jmc.stanford.edu/articles/lisp/lisp.pdf>:

> Lisp was first implemented by Steve Russell on an IBM 704 computer
> using punched cards. Russell had read McCarthy's paper and
> realized (to McCarthy's surprise) that the Lisp eval function could
> be implemented in machine code. 

1. Obviously, we can replace Lisp by any other language.

2. Actually, we
   only need this interpreter for a subset of the language large enough
   to write a simple-minded compiler in it. For subsequent steps, we can
   use the thus obtained Lisp->ML compiler and extend it gradually.
   (That's incidentally the topic of section 4.)

3. Suppose we created the Lisp->ML compiler in ML as indicated at end of
   section 3. How can we check its correctness? We can check it
   against the Lisp->ML compiler in Lisp: the generated code should
   be literally the same! Stress test: compile Lisp->ML in Lisp using
   Lisp->ML in ML. That should generate a literal copy of Lisp->ML in
   ML. (If you want to see this in action, install the OCaml system
   from source! It executes all these steps.)

## compiler-compiler, take 2

Another approach to a compiler-compiler, potentially inspired by the
Lisp example.

### The Futamura Projections  
 
The starting point here is an interpreter for a language L written in
a metalanguage M. Clearly, the interpreter specifies the semantics of
L as it computes the output of every L-program `pL` and input `in` as
follows:

	M[intL](pL, in) = out

Now suppose, we have a *partial evaluator* `spec` for M programs. This
partial evaluator is specified as follows for an M-program `pM` that
takes two inputs `ins` and `ind`:

	M[pM] (ins, ind) = out-pm

Now we apply the partial evaluator to `pM` and the first input `ins`
to obtain a *specialized program* `pins`:

	M[spec] (pM, ins) = pins

such that the specialized program produces the same result:

	M[pins] (ind) = out-pm

The **first Futamura projection**: Now, we can apply this tool to `p = intL` and `ins = pL`:

	M[spec] (intL, pL) = int-p

The result is *the interpreter specialized to the program `pL`* because

	M[int-p] (in) = out

So in essence, we have *compiled* `pL` into M, because `int-p` is an
M-program that behaves just like `pL`!


The **second Futamura projection**:
We can further exploit the fact that `spec` itself is an M-program
that incidentally takes two arguments. In particular, we might try and
apply `spec` to itself! Any M-program is a suitable argument for
`spec`, so we choose `intL` as an argument:

	M[spec] (spec, intL) = spec-int

Now `spec-int` is a very interesting program! If we apply it to a
program like `pL` we obtain the compiled program from above:

	M[spec-int] (pL) = int-p

Thus, `spec-int` acts like a compiler!

The **third Futamura projection** takes this idea to the extreme by
iterating the specialization once more.

	M[spec] (spec, spec) = cogen

Let's see what we can do with this program. It should take an
M-program as input, e.g., `intL`. According to our previous
experiments, the resulting specialized program should be the program `spec-int`:

	M[cogen] (intL) = spec-int

Previously, we realized that `spec-int` acts like an L->M compiler, so
`cogen` can be considered a compiler-compiler (or compiler generator,
hence the name `cogen`). 

The main question is whether such a program `spec` exists that can be
applied to itself and deliver meaningful results! This question has
been answered affirmatively in:

* Neil D. Jones, Peter Sestoft, Harald SÃ¸ndergaard.
	An Experiment in Partial Evaluation: The Generation of a Compiler Generator. RTA 1985: 124-140.

# 4. Bootstrapping


Two kinds of bootstrapping
* porting a compiler from one machine to another (starting with cross
  compilation),
* growing a language by stepwise extension with new features.

The compiler (CWL->ML2) in ML1 is a *cross compiler* because it runs
on ML1 and generates code for ML2. Typical situation for embedded
systems, which are too resource limited to run a compiler. 

Second kind of bootstrapping.
1. obtain COMP[i+1] : CWL[i+1]->ML by compiling using COMP[i] : CWL[i]->ML.
2. observe that COMP[i+1] is written in the language processed by
   COMP[i], which is CWL[i].
3. We wish to upgrade that to CWL[i+1], so we rewrite COMP[i+1] to
   COMP*[i+1] which also uses CWL[i+1] features.
4. Then compile COMP*[i+1] using COMP[i+1] to obtain a compiler that
   is written in CWL[i+1] and that also compiles CWL[i+1] -> ML. So it
   can compile itself!

This example is given in terms of CWL a compiler-writing language, but
it also applies to any programming language. You could start
implementing a C0 compiler, say, in some language, and then iterate in
exactly the same way to obtain a full C99 compiler.

The alternative tower of interpreters approach is much, much more
inefficient. Each layer of interpretation costs about an order of
magnitude, so it's really impractical.

# assigned reading: none

* frontend: lexing
